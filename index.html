
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="seal_icon.png">
  <title>Skand</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>

  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9HPB2N5GPK"></script>
  <script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'G-9HPB2N5GPK');
  </script>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <script type="text/javascript" src="js/hidebib.js"></script>
          <name>Skand</name><br>
          <b>Email</b>:
          <font id="email" style="display:inline;">
            <noscript><i>Please enable Javascript to view</i></noscript>
          </font>
          <script>
          emailScramble = new scrambledString(document.getElementById('email'),
              'emailScramble', 'nmml.kopisaor@onvcapdt',
              [6, 22, 15, 18, 19, 4, 11, 1, 17, 3, 16, 13, 10, 8, 21, 14, 2, 20, 5, 9, 7, 12]);
    </script>
        </p>
        <p>I am an incoming Ph.D. student at Oregon State University. My primary research interests lie at the intersection of Machine Learning, Robotics and Cognitive Science. <br> <br>
        Prior to this, I was working with <a href="http://www.sungjinahn.com/" target="_blank">Sungjin Ahn</a> on object-centric world models and reinforcement learning.
        Earlier, I was a Research Assistant at the Indian Institute of Science working with 
           <a href="http://cds.iisc.ac.in/faculty/venky/" target="_blank"> Venkatesh Babu</a>.
          I graduated with Bachelors of Technology (B.Tech) degree in Computer Science and Engineering from <a href="http://www.iitrpr.ac.in/" target="_blank">
            Indian Institute of Technology, Ropar, India</a>.
        </p>
        <p>
          I  was advised by
           <a href="http://cse.iitrpr.ac.in/ckn/index.html" target="_blank">Narayanan. C. Krishnan</a> for my Bachelor's Thesis. During my undergrad, I also worked with <a href="http://cse.iitrpr.ac.in/deepti/team/" target="_blank"> Deepti Bathula </a> and
          <a href="https://sites.google.com/site/dhallabhinav/" target="_blank"> Abhinav Dhall </a>.
          <br>
          I previously interned at <a href="http://deepchem.io/" target="_blank"> DeepChem </a> (GSoC) and 
          <a href="http://val.serc.iisc.ernet.in/valweb/" target="_blank">Video Analytics Lab, IISc Bangalore</a>.
        </p>

        <p align=center>

          <a href="http://www.github.com/pvskand" target="_blank"> Github </a>&nbsp/&nbsp
          <a href="http://www.twitter.com/pvskand" target="_blank"> Twitter </a>&nbsp/&nbsp
          <a href="https://scholar.google.co.in/citations?user=AaY4U-wAAAAJ&hl=en" target="_blank"> Google Scholar </a>
        </p>
        </td>
        <td width="33%">
        <img src="images/skand-circle.png" width="100%">
        </td>
      </tr>
      </table>


      <!-- PUBLICATIONS -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr><td><heading>Pre-Prints / Publications</heading></td></tr>
      </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            
            <tr>
              <td width="33%" valign="top" align="center"><img src="images/swb.png" style="border-color:black" width="200" height="200"><hr style="height:0pt; visibility:hidden; margin:0"/>
              <td width="67%" valign="top">
                <p id="SWB">
                <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
                <heading style="color:#1772d0;">Structured World Belief for Reinforcement Learning in POMDP
                </heading></a><br>
                Gautam Singh, <strong>Skand</strong>, Junghyun Kim, Hyunseok Kim, Sungjin Ahn<br>
                <em>In International Conference on Machine Learning (ICML)</em>, 2021.<br>
                </p>

                <div class="paper" id="swb">
                <a href="javascript:toggleblock('swb_abs')">abstract</a> |
                <a shape="rect" href="javascript:togglebib('swb')" class="togglebib">bibtex</a> |
                <a href="pdf/SWB.pdf">paper</a>
                <br>

                <p align="justify"> <i id="swb_abs"> Object-centric world models provide structured representation of the scene and can be an important backbone in reinforcement learning and planning. However, existing approaches suffer in partially-observable environments due to the lack of belief states. In this paper, we propose Structured World Belief, a model for learning and inference of object-centric belief states. Inferred by Sequential Monte Carlo (SMC), our belief states provide multiple object-centric scene hypotheses. To synergize the benefits of SMC particles with object representations, we also propose a new object-centric dynamics model that considers the inductive bias of object permanence. This enables tracking of object states even when they are invisible for a long time. To further facilitate object tracking in this regime, we allow our model to attend flexibly to any spatial location in the image which was restricted in previous models. In experiments, we show that object-centric belief provides a more accurate and robust performance for filtering and generation. Furthermore, we show the efficacy of structured world belief in improving the performance of reinforcement learning, planning and supervised reasoning.
                </i></p>
                <pre xml:space="preserve">
                    @inproceedings{SWB21,
                      Author = {Singh, Gautam and Skand and Kim, Junghyun and Kim, Hyunseok and Ahn, Sungjin},
                      Title = {Structured World Belief for Reinforcement Learning in POMDP},
                      Booktitle = {ICML},
                      Year = {2021}
                  }
                </pre>
                </div>
              </td>
            </tr>

            <tr>
              <td width="33%" valign="top" align="center"><img src="images/gswm.gif" style="border-color:black" width="220" height="120"><hr style="height:0pt; visibility:hidden; margin:0"/>
              <td width="67%" valign="top">
                <p id="GSWM">
                <heading style="color:#1772d0;">Improving Generative Imagination in Object-Centric World Models
                </heading></a><br>
                Zhixuan Lin, Yi-Fu Wu, <strong>Skand</strong>, Bofeng Fu, Jindong Jiang, Sungjin Ahn<br>
                <em>In International Conference on Machine Learning (ICML)</em>, 2020.<br>
                </p>

                <div class="paper" id="gswm">
                <a href="javascript:toggleblock('gswm_abs')">abstract</a> |
                <a shape="rect" href="javascript:togglebib('gswm')" class="togglebib">bibtex</a> |
                <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/4995-Paper.pdf">paper</a> |
                <a href="https://sites.google.com/view/gswm/home">Project Page</a> |
                <a href="https://github.com/zhixuan-lin/G-SWM">Code</a> 
                <br>

                <p align="justify"> <i id="gswm_abs"> The remarkable recent advances in object-centric generative world models raise a few questions. First, while many of the recent achievements are indispensable for making a general and versatile world model, it is quite unclear how these ingredients can be integrated into a unified framework. Second, despite using generative objectives, abilities for object detection and tracking are mainly investigated, leaving the crucial ability of temporal imagination largely under question. Third, a few key abilities for more faithful temporal imagination such as multimodal uncertainty and situationawareness are missing. In this paper, we introduce Generative Structured World Models (G-SWM). The G-SWM achieves the versatile world modeling not only by unifying the key properties of previous models in a principled framework but also by achieving two crucial new abilities, multimodal uncertainty and situation-awareness. Our thorough investigation on the temporal generation ability in comparison to the previous models demonstrates that G-SWM achieves the versatility with the best or comparable performance for all experiment settings including a few complex settings that have not been tested before.
                </i></p>
                <pre xml:space="preserve">
                    @inproceedings{SPACE20,
                      Author = {Lin, Zhixuan and Wu, Yi-Fu and Skand and Fu, Bofeng and Jiang, Jindong and Ahn, Sungjin},
                      Title = {Improving Generative Imagination in Object-Centric World Models},
                      Booktitle = {ICML},
                      Year = {2020}
                  }
                </pre>
                </div>
              </td>
            </tr>

            <tr>
              <td width="33%" valign="top" align="center"><img src="images/space.gif" style="border-color:black" width="220" height="150"><hr style="height:0pt; visibility:hidden; margin:0"/>
              <td width="67%" valign="top">
                <p id="DDCNN">
                <heading style="color:#1772d0;">SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition</heading></a><br>
                Zhixuan Lin*, Yi-Fu Wu*, <strong>Skand*</strong>, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, Sungjin Ahn<br>
                <em>(* Equal Contribution)</em> <br>
                <em>In International Conference on Learning Representations (ICLR)</em>, 2020.<br>
                </p>

                <div class="paper" id="space">
                <a href="javascript:toggleblock('space_abs')">abstract</a> |
                <a shape="rect" href="javascript:togglebib('space')" class="togglebib">bibtex</a> |
                <a href="https://arxiv.org/pdf/2001.02407.pdf">arXiv</a> |
                <a href="https://sites.google.com/view/space-project-page">Project Page</a> |
                <a href="https://openreview.net/forum?id=rkl03ySYDH">OpenReview</a> | 
                <a href="https://github.com/zhixuan-lin/SPACE">Code</a> 
                <br>

                <p align="justify"> <i id="space_abs"> The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a uniﬁed probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS.
                </i></p>
                <pre xml:space="preserve">
                    @inproceedings{SPACE20,
                      Author = {Lin, Zhixuan and Wu, Yi-Fu and Skand and Sun, Weihao and Singh, Gautam and Deng, Fei and Jiang, Jindong and Ahn, Sungjin},
                      Title = {SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition},
                      Booktitle = {ICLR},
                      Year = {2020}
                  }
                </pre>
                </div>
              </td>
            </tr>

          <tr>
              <td width="33%" valign="top" align="center"><img src="images/lsccnn.png" style="border-color:black" width="220" height="150"><hr style="height:0pt; visibility:hidden; margin:0"/>
              <td width="67%" valign="top">
                <p id="LSCCNN">
                <heading style="color:#1772d0;"> Locate, Size and Count: Accurately Resolving People in Dense Crowds via Detection</heading></a><br>
                Deepak Babu Sam*, <strong>Skand*</strong>, Mukuntha Narayanan Sundararaman, Amogh Kamath, R. Venkatesh Babu<br>
                <em>(* Equal Contribution)</em> <br>
                <em>In Transaction on Pattern Analysis and Machine Intelligence (T-PAMI)</em>, 2020.<br>
                </p>

                <div class="paper" id="lsccnn">
                <a href="javascript:toggleblock('lsccnn_abs')">abstract</a> |
                <a shape="rect" href="javascript:togglebib('lsccnn')" class="togglebib">bibtex</a> |
                <a href="https://arxiv.org/abs/1906.07538" target="_blank">arXiv</a> | 
                <a href="https://github.com/val-iisc/lsc-cnn" target="="_blank>code</a> |
                <a href="notes/privacy_note.txt" target="="_blank>NOTE</a>
                <br>

                <p align="justify"> <i id="lsccnn_abs"> We introduce a detection framework for dense crowd counting and eliminate the need for the prevalent density regression paradigm. Typical counting models predict crowd density for an image as opposed to detecting every person. These regression methods, in general, fail to localize persons accurate enough for most applications other than counting. Hence, we adopt an architecture that locates every person in the crowd, sizes the spotted heads with bounding box and then counts them. Compared to normal object or face detectors, there exist certain unique challenges in designing such a detection system. Some of them are direct consequences of the huge diversity in dense crowds along with the need to predict boxes contiguously. We solve these issues and develop our LSC-CNN model, which can reliably detect heads of people across sparse to dense crowds. LSC-CNN employs a multi-column architecture with top-down feedback processing to better resolve persons and produce refined predictions at multiple resolutions. Interestingly, the proposed training regime requires only point head annotation, but can estimate approximate size information of heads. We show that LSC-CNN not only has superior localization than existing density regressors, but outperforms in counting as well.
                </i></p>
                <pre xml:space="preserve">
                    @article{LSCCNN,
                      Author = {Sam, Deepak Babu and Skand and M. N. Sundararaman,  and Kamath, Amogh and Babu, R. Venkatesh},
                      Title = {Locate, Size and Count: Accurately Resolving People in Dense Crowds via Detection},
                      Journal = {IEEE TPAMI},
                      Year = {2020}
                  }
                </pre>
                </div>
              </td>
            </tr>
            
            <tr>
              <td width="33%" valign="top" align="center"><img src="images/ddcnn.png" style="border-color:black" width="220" height="150"><hr style="height:0pt; visibility:hidden; margin:0"/>
              <td width="67%" valign="top">
                <p id="DDCNN">
                <heading style="color:#1772d0;">Going Beyond the Regression Paradigm with Accurate Dot Prediction for Dense Crowds</heading></a><br>
                Deepak Babu Sam*, <strong>Skand*</strong>, Mukuntha Narayanan Sundararaman, R. Venkatesh Babu<br>
                <em>(* Equal Contribution)</em> <br>
                <em>In Winter Conference on Applications of Computer Vision (WACV)</em>, 2020.<br>
                </p>

                <div class="paper" id="ddcnn">
                <a href="javascript:toggleblock('ddcnn_abs')">abstract</a> |
                <a shape="rect" href="javascript:togglebib('ddcnn')" class="togglebib">bibtex</a> |
                <a href="pdf/wacv/DDCNN_WACV_2020.pdf">paper</a> |
                <a href="notes/privacy_note.txt" target="="_blank>NOTE</a>
                <br>

                <p align="justify"> <i id="ddcnn_abs"> We propose a radical shift to the paradigm of density regression widely being employed for tackling crowd counting. In the prevalent regression approach, a model is trained for mapping images to its crowd density rather than counting by detecting every person. This framework is motivated from the difficulty to discriminate humans in highly dense crowds where unfavorable perspective, occlusion and clutter are prevalent. Though regression methods estimate overall crowd counts pretty well, localization of individual persons suffers and varies considerably across the entire density spectrum. Moreover, individual detection of people aids more explainable practical systems than predicting blind crowd count or density map. Hence, we move away from density regression and reformulate the task as localized dot prediction in dense crowds. Our dot detection model, DD-CNN, is trained for pixel-wise binary classification to detect people instead of regressing local crowd density. In order to handle severe scale variation and detect people of all scales with accurate dots, we use a novel multi-scale architecture which does not require any ground truth scale information. This training regime, which incorporates top-down feedback, helps our model to localize people in sparse as well as dense crowds. Our model delivers superior counting performance on major crowd datasets. We also evaluate on some additional metrics and evidence superior localization of the dot detection formulation.
                </i></p>
                <pre xml:space="preserve">
                    @inproceedings{DDCNN20,
                      Author = {Sam, Deepak Babu and Skand and M. N. Sundararaman , and Babu, R. Venkatesh},
                      Title = {Going Beyond the Regression Paradigm with Accurate Dot Prediction for Dense Crowds},
                      Booktitle = {WACV},
                      Year = {2020}
                  }
                </pre>
                </div>
              </td>
            </tr>
    
            <tr>
              <td width="33%" valign="top" align="center"><img src="images/miccai18.png" style="border-color:black" width="220" height="150"><hr style="height:0pt; visibility:hidden; margin:0"/>
              <td width="67%" valign="top">
                <p id="MICCAI18">
                <heading style="color:#1772d0;"> MRI to FDG-PET: Cross-Modal Synthesis Using 3D U-Net For Multi-Modal Alzheimer's Classification</heading></a><br>
                Apoorva Sikka, <strong>Skand</strong>, Deepti.R.Bathula<br>
                <em>In MICCAI Workshop on Simulation and Synthesis in Medical Imaging</em>, 2018.<br>
                </p>

                <div class="paper" id="miccai18">
                <a href="javascript:toggleblock('miccai_abs')">abstract</a> |
                <a shape="rect" href="javascript:togglebib('miccai18')" class="togglebib">bibtex</a> |
                <a href="https://arxiv.org/abs/1807.10111" target="_blank">arXiv</a>
                <br>

                <p align="justify"> <i id="miccai_abs"> Recent studies suggest that combined analysis of Magnetic resonance imaging~(MRI) that measures brain atrophy and positron emission tomography~(PET) that quantifies hypo-metabolism provides improved accuracy in diagnosing Alzheimer's disease. However, such techniques are limited by the availability of corresponding scans of each modality. Current work focuses on a cross-modal approach to estimate FDG-PET scans for the given MR scans using a 3D U-Net architecture. The use of the complete MR image instead of a local patch based approach helps in capturing non-local and non-linear correlations between MRI and PET modalities. The quality of the estimated PET scans is measured using quantitative metrics such as MAE, PSNR and SSIM. The efficacy of the proposed method is evaluated in the context of Alzheimer's disease classification. The accuracy using only MRI is 70.18% while joint classification using synthesized PET and MRI is 74.43% with a p-value of 0.06. The significant improvement in diagnosis demonstrates the utility of the synthesized PET scans for multi-modal analysis.
                </i></p>


                <pre xml:space="preserve">
                @inproceedings{MRI2PET_MICCAI,
                    Author = {Sikka, Apoorva and Skand and Bathula, Deepti.R},
                    Title = { MRI to FDG-PET: Cross-Modal Synthesis Using 3D U-Net For Multi-Modal Alzheimer's Classification},
                    Booktitle = {MICCAI Workshop on Simulation and Synthesis in Medical Imaging},
                    Year = {2018}
                }
                </pre>
                </div>
              </td>
            </tr>

            <tr>
              <td width="33%" valign="top" align="center"><img src="images/cavinet.jpg" style="border-color:black" width="220" height="150"><hr style="height:0pt; visibility:hidden; margin:0"/>
              <td width="67%" valign="top">
                <p id="ACMMM18">
                <heading style="color:#1772d0;">Deep Cross modal learning for Caricature Verification and Identification (CaVINet)</heading></a><br>
                Jatin Garg*, <strong>Skand Peri*</strong>, Himanshu Tolani*, Narayanan.C.Krishnan<br>
                <em>(* Equal Contribution, and names sorted alphabetically)</em> <br>
                <em>ACM Multimedia</em>, 2018.<br>
                </p>

                <div class="paper" id="acmmm18">
                <a href="javascript:toggleblock('acmmm18_abs')">abstract</a> |
                <a shape="rect" href="javascript:togglebib('acmmm18')" class="togglebib">bibtex</a> |
                <a href="https://arxiv.org/abs/1807.11688" target="_blank">arXiv</a> |
                <a href="https://lsaiml.github.io/CaVINet/" target="_blank">project page </a>
                <br>

                <p align="justify"> <i id="acmmm18_abs"> Learning from different modalities is a challenging task that involves determining a shared space that bridges the two modalities. In this paper, we look at the challenging problem of cross modal face verification and recognition between caricature and visual image modalities. Caricature is a modality with images having exaggerations of facial features of a person. Due to the significant variations in the caricatures, building vision models for recognizing and verifying data from this modality is an extremely challenging task. Visual images with significantly lesser amount of distortions can act as a bridge for the analysis of caricature modality. To advance the research in this field, we have created a publicly available large Caricature-VIsual dataset [CaVI] with images from both the modalities. The dataset captures the rich variations in the caricature of an identity. This paper presents the first cross modal architecture that is able to handle extreme distortions present in caricatures using a deep learning network that learns similar representations across the modalities. We use two convolutional networks along with transformations that are subjected to orthogonality constraints to capture the shared and modality specific representations. In contrast to prior research, our approach neither depends on manually extracted facial landmarks for learning the representations, nor on the identities of the person for performing verification. The learned shared representation achieves 91% accuracy for verifying unseen images and 75% accuracy on unseen identities. Further, recognizing the identity in the image by knowledge transfer using a combination of shared and modality specific representations, resulted in an unprecedented performance of 85% rank-1 accuracy for caricatures and 95% rank-1 accuracy for visual images.
                 </i></p>


                <pre xml:space="preserve">
                @inproceedings{CaVINet_ACMMM18,
                    Author = {Garg, Jatin and Skand and Tolani, Himanshu and
                    Krishnan, Narayana.C},
                    Title = {Deep Cross modal learning for Caricature Verification and Identification (CaVINet)},
                    Booktitle = {ACM Multimedia},
                    Year = {2018}
                }
                </pre>
                </div>
              </td>
            </tr>

            <tr>
              <td width="33%" valign="top" align="center"><img src="images/disguisenet.jpg" style="border-color:black" width="220" height="150"><hr style="height:0pt; visibility:hidden; margin:0"/>
              <td width="67%" valign="top">
                <p id="CVPRW18">
                <heading style="color:#1772d0;">DisguiseNet : A Contrastive Approach for Disguised Face Verification in the Wild</heading></a><br>
                <strong>Skand</strong>, Abhinav Dhall<br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition, Workshop on Disguised Faces in the Wild (CVPRW)</em>, 2018<br>
                </p>

                <div class="paper" id="cvprw18">
                <a href="javascript:toggleblock('cvprw18_abs')">abstract</a> |
                <a shape="rect" href="javascript:togglebib('cvprw18')" class="togglebib">bibtex</a> |
                <a href="https://arxiv.org/abs/1804.09669" target="_blank">arXiv</a> |
                <a href="https://github.com/pvskand/DisguiseNet" target="_blank">code</a>
                <br>

                <p align="justify"> <i id="cvprw18_abs">This paper describes our approach for the Disguised Faces in the Wild (DFW) 2018 challenge. The task here is to verify the identity of a person among disguised and impostors images. Given the importance of the task of face verification it is essential to compare methods across a common platform. Our approach is based on VGG-face architecture paired with Contrastive loss based on cosine distance metric. For augmenting the data set, we source more data from the internet. The experiments show the effectiveness of the approach on the DFW data. We show that adding extra data to the DFW dataset with noisy labels also helps in increasing the generalization performance of the network. The proposed network achieves 27.13% absolute increase in accuracy over the DFW baseline.</i></p>

                <pre xml:space="preserve">
                @inproceedings{periCVPRW,
                    Author = {Skand and
                    Dhall, Abhinav},
                    Title = {DisguiseNet : A Contrastive Approach for Disguised Face Verification in the Wild},
                    Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
                    Year = {2018}
                }
                </pre>
                </div>
              </td>
            </tr>

      </table>



      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">

          Template : <a href="http://jeffdonahue.com/">this</a>, <a href="https://jonbarron.info/">this</a>, <a href="http://people.eecs.berkeley.edu/~pathak/">this</a> and <a href="https://people.eecs.berkeley.edu/~sgupta/">this</a>
	    </font>
        </p>
        </td>
      </tr>
      </table>

    </td>
    </tr>
  </table>


</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvprw18_abs');
hideblock('acmmm18_abs');
hideblock('miccai_abs');
hideblock('lsccnn_abs');
hideblock('ddcnn_abs');
hideblock('space_abs');
hideblock('gswm_abs');
hideblock('swb_abs');
</script>




  </body>
</html>
